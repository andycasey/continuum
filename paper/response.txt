# Reviewer Report Responses

We thank the referee for their review, and we apologize for the lateness of our reply. We have addressed all queries put forward by the reviewer.

---

## Abstract

> **Line 27**: "Most unphysical spectra": I did not find in the paper an explanation of this sentence. Particularly, I am curious about the exceptions in which - despite the constraints - unphysical spectra are obtained.

Clarified that the constraints prohibit rectified flux values exceeding unity.

> **Lines 29-30**: For the sake of completeness the description of the model should mention that there is a training required, and potentially some arbitrarity in meta parameters (e.g. K).

The description does describe how "stellar absorption and telluric transmission are each modelled by factorising a grid of rectified theoretical spectra", and we have added a note about the number of basis components.

> **Line 31**: Since radial velocities and broadening have not been demonstrated in the paper, this should be removed from the brackets. The authors are welcome to mention that an extension of the model to include those is possible.

Changed to indicate these are extensions: "and is readily extensible to radial velocity and rotational broadening."

> **Line 35**: To clarify this result already at abstract level I suggest a slightly expanded phrasing, e.g. "consistent across time to 0.3% at S/N ~ 100, and to better than 0.4% at S/N ~ 30".

Changed to: "consistent across time to 0.2% at S/N ~ 100, and to better than 0.5% at S/N ~ 30."

---

## Introduction

> **Paragraph 49**: The difference between a "classical" and an "industrial" spectroscopist is not obvious to me. I suggest to explicitly state which are the different kinds of applications that have different requirements for continuum normalization (e.g. applications to individual objects to estimate accurate atmospheric parameters, extraction of a large number of normalized spectra from a survey, ...).

Expanded to explicitly describe the two use cases: (1) detailed studies of individual objects for accurate atmospheric parameters/abundances, and (2) large spectroscopic surveys requiring consistent automatic extraction.

---

## Methods

> **Figure 1**: it is not quoted in the methods section, and quoted only in Sec. 4, so it's possibly misplaced.

Added reference to Figure 1 where basis vectors are first introduced.

> **Line 128**: Does bold-face indicate an array? Please specify.

Added: "Throughout this work, bold-face symbols denote vectors or matrices (see Table 1)."

> **Line 130**: I suspect a mistake in the coefficient to the third order Taylor expansion. Should it be 2/6?

There was an error in the Taylor series expansion, but we have also now limited it to second-order approximation, as this is more than necessary for most astronomical applications.

> **Line 130 (separate)**: Separate to previous point: why is it necessary to expand in series to the fourth order? For the average astronomer, the error on log(x) is dx/x (me included, prior to thinking about it).

The reviewer is right: it's not! We have limited it to a second-order approximation.

> **Line 134**: For consistency with the rest, I suggest boldfacing lambda in Eq. (3).

Done.

> **M 138**: The discussion on the precision vs accuracy of the continuum is important in this paper. I therefore find it necessary to spend a few words about what is meant by "continuum-normalized theoretical spectra". This might seem obvious at first glance, but the wings of a broad line (e.g. hydrogen lines) might be considered as part of the continuum when interested in the abundance of a different species (e.g. iron). The normalization of theoretical spectra is supposed to be "perfect", but is it truly an unambiguous choice? How exactly is the theoretical normalization performed by the authors?

Added detailed explanation: the rectified flux from synthesis codes is the emergent flux divided by the theoretical continuum (flux in absence of line opacity). This normalization is unambiguous and performed by the synthesis code itself.

> **Lines 141-143**: It seems that the line spread function is accounted for in this work by convolving the model with a kernel (line 187). This sentence seems to state that this is not necessary, which creates some confusion.

Clarified: the training grid need not match data resolution because basis vectors can be convolved to instrument resolution at inference time, but we have noted that it can be helpful if they are pre-convolved.

> **Line 147**: The reader might benefit from an additional explanation of why it is convenient to move to a sparse matrix.

Added explanation: NMF efficiently exploits sparsity, allowing the factorisation to focus on learning absorption structure rather than modelling near-unity continuum values.

> **Lines 148-150**: Is this transformation the NMF itself? Some of the reasons for this choice of transformation should be listed for clarity (or different parts of the paper that discuss them pointed at).

Added three reasons: (1) converts multiplicative to additive model; (2) ensures non-negativity for NMF requirements; (3) produces sparse matrices.

> **M 154**: How is the number of basis components (K) chosen? Is this choice critical to the success of the method?

Added explanation: we select by monitoring reconstruction loss on validation set. Also added specific justification in Experiments section.

> **Lines 165-167**: Here or elsewhere it would be interesting to have an estimate of the computational time required for the training with the chosen grid of models (how many are they?), and of execution time to fit an individual spectrum.

Added: training takes ~30 minutes on single CPU for millions of spectra, inference takes a few core-seconds per HARPS spectrum.

> **Line 171**: Is there any requirement in terms of spectral resolution of the models? For typical applications in the spectral fits with which I am familiar, a resolution of about 10 times higher than the instrumentation resolving power is required.

Not strictly. Added: The native resolution of the theoretical grid should be at least comparable to the target instrument resolution.

> **M 190**: To my knowledge, the ATLAS9 code that lies at the basis of the BOSZ models is primarily designed to treat photospheric conditions in LTE, where no emission lines are expected. I am therefore wondering why do the authors have some flux values exceeding 1 after normalizing by the theoretical continuum.

Clarified: these occur rarely due to numerical precision in synthesis codes or interpolation, not physical emission (since it is an LTE code).

> **Line 195**: I do not find the derivation of this equation trivial, despite finding it rather intuitive. Do the authors mean that this mathematically follows from Eq. (3) and (4)?

Added explicit derivation from Equation 4 to Equation 5, which leads on to 6-7, and then to Eqs 8-10.

> **Line 202**: The implication that non-negativity of F and alpha allows h to properly fit for the continuum is not obvious, please expand a bit on it.

Added mathematical explanation: since F >= 0 and alpha >= 0, we have f <= 1, so any flux above the stellar model must be captured by continuum.

> **Line 248**: I suggest to define the quantities with hat for additional clarity.

Added explanation that hats denote optimised parameters.

---

## Experiments

> **Line 277**: I suggest to include a table that summarizes The S/N for each star in the selected sample.

Added Table 2 with star names, spectral types, approximate S/N ratios, and notes.

> **Line 279**: To clarify: is the convolution performed after constructing the basis? Would there be a difference in constructing the basis prior to applying the convolution?

Clarified: For these stars, convolution is applied to the already-learned basis vectors, before solving for the non-negative weights.

> **M 295**: I suggest to test the pipeline on different spectra for an A and B stars that are unaffected by ghosts/reflections. Since these spectral types end up being treated differently from the others to have a good result, this additional test would be a clean way to show that this is indeed due to a specificity of the two datasets chosen and not due to a difficulty of the pipeline with, e.g., fast rotating hot stars.

We considered this at length before submission. Ultimately, the artefacts are observation-specific rather than dependent on particular spectral types, and through discussions with colleagues they commented on how it was useful to know that the method could be applied to spectra that were both blaze-corrected or not. For this reason, we have kept these examples and explicitly noted in the text "We verified that the blaze correction is specific to these particular observations and not a general limitation for hot stars by successfully fitting other A- and B-type stars from the archive without blaze correction; the artefacts appear to be observation-specific rather than spectral-type-dependent, and we leave these examples in to demonstrate the applicability of our method to both blaze-corrected and blaze-uncorrected spectra."

> **M 305**: Additional details on the choice of these meta-parameters would be really appreciated (see also comment on line 154).

Added justification for $K$ of 16, $L$ of 4, and $F$ of 9.

> **Line 309**: There is a degree of redundancy between the sentence "In this experiment ..." and the more precise description at Lin 316.

Removed redundant sentence.

---

## Results

> **M Figure 2**: A few insets showing zooms of Figure 2 would be appreciated. It would be interesting to see both regions where the model fit is particularly successful, and where it is less successful (e.g. following the discussion at lines 349 - 351, and line 358).

Added insets to Figure 2 and updated the caption accordingly.

> **M 325 - 343**: I tend to agree with the optimism of the manuscript, but it would be useful to provide a few more quantitative arguments about the performance. Looking at Figure 2 there is a significant number of pixels that exceeds the dashed line, which I assume is the rectified continuum. Is this consistent with the photon noise distribution + emission lines? To my eye, it seems that especially for the colder spectral types there might be errors of a few % (estimated from the max excess of flux beyond 1). This is also visible in Fig. 2, with many data-pixels exceeding all model components. This precision might be sufficient for the scope of the tool and might be best-in-class, but I urge the authors to attempt their best at quantifying this. For instance, at line 340 - 341 it is mentioned that the theoretical expectation for the continuum model matches the retrieved one. This should quantified (e.g. squared residuals) and shown with a specific figure, and possibly extended to all spectral types covered.

Added some discussion about this point: If the theoretical models were perfect descriptions of stellar spectra, and our NMF approximation was also perfect, then any pixels exceeding the dashed line in Figure 3 should be consistent with photon noise and emission lines. However, neither is true. This motivates that we should add flexibility to our stellar absorption model by increasing $K$ or building the stellar absorption model from observed spectral templates.

> **Lines 352-353**: This is not obvious for the M1 to F6 stars blue-ward of ~4500 Angstroms. Please elaborate on the argument about this specific region.

Added clarification that this region presents a greater challenge due to high density of absorption features.

> **Line 357**: Why not use ESO SkyCalc?

We have added an explanation that this was largely due to convenience rather than for optimal purposes. In practice we found ESO SkyCalc very difficult to install and use. We would have found it necessary to compute a large grid of telluric spectra with different combinations of atmospheric contributions, which was cumbersome. High-resolution templates would improve our fits. The method is capable of using any kind of telluric grid.

> **Line 362**: Could you show exactly what you mean? This would be clear from an analogous figure to Fig. 1 but for tellurics and continuum basis, which I suggest to add.

Added: The telluric basis vectors describe narrow atmospheric features while the continuum basis vectors only includes smooth functions.

> **M 370 - 381 and Figure 4**: The metric presented in Figure 4 require some clarifications.
1) There is no line visible in Fig. 4.
2) What is meant by "normalised flux"? I did not find a definition.
3) From Sec. 3 I expected a total of 20 spectra (one per bin in average S/N per pixel), hence potentially 20 points in the summary statistic presented in Fig. 4, but I see that the x-axis of the Figure is much more densely populated. This makes me think that S/N per pixel has a different meaning here compared to Sec. 3, which should be clarified. What is the x-axis exactly?
To try and illustrate my confusion, my interpretation would be that in Figure 4 the x-axis represents the SNR in all individual pixels in the spectra considered, but I can not reconcile this with the fact that there should be stellar lines in many pixels which do not appear in Figure 4. Does this only represent the value of the continuum extracted by the NMF method, or in other words Eq. (16)?

We apologize for the confusion. Originally we used 20 spectra but found the sampling too course. Instead we took all spectra with S/N < 200 and we show the results from all spectra (with a S/N bin size of 1: up to 200 spectra if all S/N were sampled). The visible line meant to describe the density in the 2D histogram.

We have extensively revised the text and clarified the S/N bin sampling.

---

## Discussion

> **M 399-402**: This discussion does not consider that synthetic stellar spectra are not perfect representations of the real stellar spectra. To some extent, residuals between models and data can be seen as "noise", but on the other hand these residuals could be captured by the "continuum" (e.g. broad-lines, dense forests of small molecular lines for later types). To some extent, I do worry that limiting the flexibility of the stellar models introduces stronger biases in the continuum to compensate for imprecise theoretical stellar spectra. What is the authors opinion about this? I think this should be briefly addressed here.

The reviewer's intuition here is correct, in that if you are simultaneously modelling things C (continuum) and S (star), and you care most about getting C correct, then you should put *all* your model capacity into S to make it as flexible as possible. In this sense, the applications we present here are more of a lower limit for the kind of performance one might expect with a NMF-based representation of line absorption. This is reflected in our updated discussion about the choice of the number of basis vectors.

We have also added some discussion about this, and limitations about how non-negativity constraint limits the stellar flexibility (e.g., continuum can only explain flux above the stellar predictions, not deficits)

> **Paragraph 404**: Paragraph 404: To my knowledge, one can always choose to use a limited number of PCA components, so I do not fully follow the argument. With a limited number of PCA components, the limitation raised by the manuscript would partially be lifted. While this truncation would be arbitrary, there seems to be a degree of arbitrarily also with NMF, which has to select the number of basis vectors. Could the authors comment on this?

Added rebuttal: even with few PCA components, the model can produce unphysical flux because weights have no sign constraint. NMF's non-negativity guarantees f <= 1 regardless of component count.

> **M 420-452**: his section of the discussion seems to place NMF somewhere in-between purely data-driven methods (e.g. with an un-interpretable basis vector as could be the case for PCA), and a fully forward-modeling approach that is less flexible than NMF. However, it is currently a little difficult to follow.
- In the first of the two paragraphs, it claims that interpretability of the basis vectors is an advantage. Since the interpretability is not fully demonstrated in this paper, but only qualitatively discussed, I suggest to tone down the first sentence of the paragraph, and refer to Casey et al., in prep.
- The first sentence of the second paragraph is a confusing as it seems to contradict that interpretability is an advantage as discussed in the preceding paragraph.
- In addition, there is no guarantee that a good fit to the data equals unbiased inference of stellar parameters, especially if the model fit is very flexible as in the case of NMF. While I don't think that the authors mean to imply this, the discussion that the comparison with existing grids might lead to biased estimates of stellar parameters is somewhat misleading since this is the case also for the NMF approach. I suggest that the authors clarify this point and add a word of caution about how to best interpret the excellent fits provided by NMF.

Toned down "highly interpretable" to "qualitatively interpretable". We added some explanation of middle-ground position regarding data-driven models and traditional forward models. We also added caution that excellent fits don't guarantee accurate parameters and shouldn't be interpreted as validation of underlying physics.

> **Lines 468-470**: Is the advantage of the constrained linear model that it would allow to evaluate the squared residuals from observations for a large number of stars, unlike traditional methods?

Added: unlike traditional forward modelling (minutes to hours per spectrum), our approach fits in seconds, enabling systematic analysis of millions of stars.

> **M 490-506**: It seems to me that this comparison with Cretignier et al. (2020) is inaccurate. The way I understand Table 2 in Cretignier et al. (2020) is that:
-The first two line entries estimate the accuracy of the continuum of a single high S/N spectrum when they compare said continuum to a theoretical prediction at the anchor points f_i.
- The rest of the line entries estimate the same quantity when - rather than a single spectrum - a whole time series is fit.
Instead, I do not see any estimation of the consistency of quantities across different spectra, if by consistency we mean "reproducibility" across time. Since the deviation between the models and the RASSINE retrieved continuum might be a systematic, the performance in terms of consistency of RASSINE might actually be better than 2%. Could the authors comment on this?

Substantially revised: clarified that Cretignier metrics represent accuracy (comparison to theoretical models), not consistency; noted that their reproducibility may be better than accuracy figures suggest; acknowledged our metrics measure different quantities.

> **Lines 521-522**: The author themselves argue that the continuum obtained with NMF is only as accurate as the (inevitably somewhat inaccurate) models used. I therefore do not find it obvious that the NMF-retrieved continuum would be more accurate than a pseudo-continuum. Could the authors explain better why they think the continuum from the NMF model is better? Is it perhaps more reproducible, or interpretable, or do they have strong reason to believe that it is indeed more accurate?

Added nuanced discussion: our estimate is preferable because (1) physically motivated, (2) reproducible without tuning, (3) works where no continuum pixels exist. However, acknowledged that expert pseudo-continuum may be more accurate in regions with poor models.

---

## Conclusions

Revised to: mention training step and hyperparameter K; add non-negativity guarantee; include quantitative consistency results; tone down interpretability claim with reference to future work.
